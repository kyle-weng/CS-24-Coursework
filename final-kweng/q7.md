# 7.
From the meltdown project, we know that time_read reads the address passed to it and returns how long it took to read it. Generally speaking, if it reads something at a particular index twice and the ratio of the time for the first read to the time for the second read is relatively low (compared to ratios for other indices), it implies that that particular "something" was already stored in cache.

In this case, the number of clocks represents on average how long it took to read each element of the array (which elements were read was determined by the stride variable-- if stride = 1, every element was read, if stride = 2, every other element was read, and so on). If "clocks" is particularly low for a given value of stride, it was probably because elements of the array were selectively cached (due to spatial locality). More explicitly, for stride = 1, the CPU probably realized that it was going to access every single element of the array and thus cached most of them, thus saving time and leading to a lower number of clocks per access.

The clock basically tells us how much time the CPU was able to save during execution by guessing which elements it was probably going to access and caching them. For most of the low values of stride, we see that the CPU is able to save a good deal of time because it correctly guessed which elements it was going to access-- this translates to relatively low clock values.

For higher values of stride, the CPU is pretty bad at guessing what elements it needs to store in cache. The elements it actually ends up accessing are more often than not absent from the cache, forcing the CPU to spend more time as it digs up the array from DRAM once again. This ultimately translates to much higher clock values. Thus, the clusters we see in the output are essentially the result of the CPU caching values it never accesses (and getting punished for it in terms of runtime).